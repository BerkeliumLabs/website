(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{354:function(t,e,a){"use strict";a.r(e);var r=a(25),s=Object(r.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"core-functions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#core-functions"}},[t._v("#")]),t._v(" Core Functions")]),t._v(" "),e("p",[t._v("Core functions are the main functions that are needed for any type of NLP task. We have defined few core functions in "),e("strong",[t._v("berkelium")]),t._v(" library.")]),t._v(" "),e("h2",{attrs:{id:"tokenizer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#tokenizer"}},[t._v("#")]),t._v(" Tokenizer")]),t._v(" "),e("p",[t._v("Tokenizer function is used to "),e("a",{attrs:{href:"https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization",target:"_blank",rel:"noopener noreferrer"}},[t._v("tokenize"),e("OutboundLink")],1),t._v(" text input and returns a "),e("code",[t._v("Array<string>")]),t._v(" of tokens. This is a core function and one of the basics steps in any NLP tasks.")]),t._v(" "),e("p",[t._v("To use tokenizer, use the following code:")]),t._v(" "),e("div",{staticClass:"language-js extra-class"},[e("pre",{pre:!0,attrs:{class:"language-js"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" tokens "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" berkelium"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("tokenize")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("h4",{attrs:{id:"parameters"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),e("p",[e("strong",[t._v("sentence")]),t._v(" "),e("code",[t._v("string")]),t._v(" A string input to be tokenized.")]),t._v(" "),e("h4",{attrs:{id:"returns"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#returns"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),e("p",[e("strong",[t._v("tokens")]),t._v(" "),e("code",[t._v("Array<string>")]),t._v(" Returns a string array of tokens.")]),t._v(" "),e("h2",{attrs:{id:"encorder"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#encorder"}},[t._v("#")]),t._v(" Encorder")]),t._v(" "),e("p",[t._v("Encoder function vectorize the string tokens. Which means, each string tokens will be assigned a unique number. This is important when preparing text data to be feed in to a machine learning model. Usually this function is used after tokenizing your string data using "),e("code",[t._v("tokenize")]),t._v(" function.")]),t._v(" "),e("p",[t._v("To vectorize the text, use the code below:")]),t._v(" "),e("div",{staticClass:"language-js extra-class"},[e("pre",{pre:!0,attrs:{class:"language-js"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" vocab "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("await")]),t._v(" berkelium"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("encode")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokens"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("h4",{attrs:{id:"parameters-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parameters-2"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),e("p",[e("strong",[t._v("tokens")]),t._v(" "),e("code",[t._v("Array<string>")]),t._v(" A string array of tokens.")]),t._v(" "),e("h4",{attrs:{id:"returns-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#returns-2"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),e("p",[e("strong",[t._v("vocab")]),t._v(" "),e("code",[t._v("DICTIONARY_BOOK")]),t._v(" An dictionary of vocabulary with their assigned unique numeric value.")]),t._v(" "),e("h2",{attrs:{id:"preprocessor"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#preprocessor"}},[t._v("#")]),t._v(" Preprocessor")]),t._v(" "),e("p",[t._v("Preprocessor function process the text data and create a "),e("code",[t._v("DATASET")]),t._v(" object that can be used to train machine learning model.")]),t._v(" "),e("p",[t._v("To preprocess the text data, use the code below:")]),t._v(" "),e("div",{staticClass:"language-js extra-class"},[e("pre",{pre:!0,attrs:{class:"language-js"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" vocab "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("await")]),t._v(" berkelium"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("preprocess")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("textData"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("h4",{attrs:{id:"parameters-3"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parameters-3"}},[t._v("#")]),t._v(" Parameters")]),t._v(" "),e("p",[e("strong",[t._v("textData")]),t._v(" "),e("code",[t._v("Array<Array<string>>")]),t._v(" We can feed the training data we prepared in "),e("RouterLink",{attrs:{to:"/v1/preparing-data.html"}},[t._v("Preparing Data")]),t._v(" step.")],1),t._v(" "),e("h4",{attrs:{id:"returns-3"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#returns-3"}},[t._v("#")]),t._v(" Returns")]),t._v(" "),e("p",[e("strong",[t._v("DATASET")]),t._v(" "),e("code",[t._v("DATASET")]),t._v(" returns a dataset object which contains following properties:")]),t._v(" "),e("ul",[e("li",[e("code",[t._v("x")]),t._v(": "),e("code",[t._v("Array<Array<number>>")]),t._v(" feature data for training (vectorized)")]),t._v(" "),e("li",[e("code",[t._v("y")]),t._v(": "),e("code",[t._v("Array<Array<number>>")]),t._v(" label data for training (vectorized)")]),t._v(" "),e("li",[e("code",[t._v("labels")]),t._v(": "),e("code",[t._v("Array<string>")]),t._v(" label data in string format")]),t._v(" "),e("li",[e("code",[t._v("vocab")]),t._v(": "),e("code",[t._v("DICTIONARY_BOOK")]),t._v(" dictionary of vocabulary found in the dataset")]),t._v(" "),e("li",[e("code",[t._v("length")]),t._v(": "),e("code",[t._v("number")]),t._v(" sequence length")])])])}),[],!1,null,null,null);e.default=s.exports}}]);